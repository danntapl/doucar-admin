# # Explore the Rides Data

# ## Setup

# Create a SparkSession
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('explore_rides').master('local').getOrCreate()


# ## Explore

# Load the rides data
rides = spark.read.csv('/duocar/raw/rides/', header=True, inferSchema=True)
# **Note:**  Consider changing the filename suffix to `tsv`.
rides.printSchema()
rides.head(5)
rides.count()

# **Obervations:**
# * id, driver_id, and rider_id are read in as integers rather than strings.
# * date_time is read in as a string rather than a timestamp.
# * Fill in missing values for service with "Standard" or "DuoCar"?
# * Round latitudes and longitudes?
# 

# **Note:** We may need to use an explicit schema or store some of these files in Parquet
# or as Hive tables in order to read in the columns as desired.  If not, we have plenty of
# potential exercises available.

# ## Inspect and cleanse ride id

# **Question:** Is `id` unique?
from pyspark.sql.functions import count, countDistinct
rides.select(count('id'), countDistinct('id')).show()

# Fix `id` column:
from pyspark.sql.functions import format_string
rides.withColumn('id', format_string('%010d', 'id')).show(5)
# **Note:** Type fields `i` and `u` do not seem to work here.

# ## Inspect and cleanse driver and rider ids

# **TODO:** Check that `driver_id` and `rider_id` are of length 12.

# **Question:** How many unique riders and drivers do we have?
rides.select(countDistinct('driver_id'), countDistinct('rider_id')).show()

# **Question:** Who are the top drivers and riders?
drivers = spark.read.csv('/duocar/raw/drivers/', header=True, inferSchema=True)
top_drivers = rides.groupby('driver_id').count().orderBy('count', ascending=False).limit(10)
top_drivers.join(drivers, top_drivers.driver_id == drivers.id, how='left').show()

# **Exercise:** Join rides and rider data and get distribution of vehicle make and model or class of service.

# ## Inspect and cleanse ride timestamp

# Fix `date_time` column:
from pyspark.sql.functions import unix_timestamp, from_unixtime
fix2 = rides.select('date_time', from_unixtime(unix_timestamp('date_time', 'yyyy-MM-dd HH:mm')).alias('date_time_fixed'))
fix2.show()

# **Question:** What is the distribution of rides during the day?
from pyspark.sql.functions import hour
fix2.select(hour('date_time_fixed').alias('hour_of_day')).groupby('hour_of_day').count().orderBy('hour_of_day').show()

# **Question:** How do we compute day of week?

# ## Inspect and cleanse type of service

# **Note:** `show` and `head` display missing values differently.
rides.select('service').show(5)
rides.select('service').head(5)

# ## Inspect and cleanse latitude and longitude

rides.describe('origin_lat', 'origin_lon', 'dest_lat', 'dest_lon').show()

# **Note:** Consider throwing in some extreme values?
# This may be more appropriate for the ride_routes data in which extreme values are generated by faulty GPS readings.

# ## Inspect and cleanse distance and duration

rides.describe('distance', 'duration').show()

# **Exercise:** Convert distance from meters to miles.

# ## Inspect and cleanse cancellation flag

rides.groupby('cancelled').count().show()

# Do missing durations correspond to cancelled rides?
rides.select(rides.duration.isNull().alias('missingDuration'), (rides.cancelled == 1).alias('rideCanceled')).crosstab('missingDuration', 'rideCanceled').show()

# **Question:** Can we predict a cancellation?

# ## Inspect and cleanse star ratings

# What is the distribution of star ratings?
rides.groupby('star_rating').count().orderBy('star_rating').show()

# **Question:** How do we compute column percents in groupby tables?
from pyspark.sql.functions import count, expr
total = rides.select(count('*').alias('N'))
rides.groupby('star_rating').count().crossJoin(total).select('star_rating', 'count', expr('round(count / N, 3)').alias('proportion')).orderBy('star_rating').show()
# Umm, that's ugly (SQL)...  And probably slow on big data...

# Plot the distribution.
import pandas as pd
import matplotlib.pyplot as plt
rides.groupBy('star_rating').count().orderBy('star_rating').toPandas().plot(kind='bar', x='star_rating', y='count')

# What is the average rating and is it meaningful?
rides.groupby().mean('star_rating').show()

# **Exercise:**  Impute a rating?


# ## Cleanup

# Stop the SparkSession
spark.stop()